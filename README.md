# Basic-Language-Model-with-Multi-Headed-Attention-Blocks
This project is essentially a way to understand and implement a Language Model with Transformers and Multi-Headed Attention Blocks. 
